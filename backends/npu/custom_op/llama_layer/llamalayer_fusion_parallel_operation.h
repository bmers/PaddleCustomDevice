/*
 * Copyright (c) Huawei Technologies Co., Ltd. 2023. All rights reserved.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
#pragma once

#include <atb/atb_infer.h>
#include <atb/svector.h>

enum LlamaLayerFusionParallelTensorId {
  IN_HIDDENSTATES = 0,
  IN_NORMWEIGHT,
  IN_QMIXDWEIGHT,
  IN_KMIXDWEIGHT,
  IN_VMIXDWEIGHT,
  IN_SELFOUTLINEARWEIGHT,
  IN_SELFOUTNORMWEIGHT,
  IN_MLPGATEWEIGHT,
  IN_MLPDOWNWEIGHT,
  IN_MLPUPWEIGHT,
  IN_POSITIONIDS,
  IN_COSTABLE,
  IN_SINTABLE,
  IN_ATTENTIONMASK,
  IN_TOKENOFFSET,
  IN_SEQLEN,
  IN_LAYERID,
  IN_CACHEK,
  IN_CACHEV,

  OUT_LLAMA13BLAYEROUT,

  INTERMIDATE_INPUTNORMOUT,
  INTERMIDATE_MIXEDQ,
  INTERMIDATE_MIXEDK,
  INTERMIDATE_MIXEDV,
  INTERMIDATE_CASTCOS,
  INTERMIDATE_CASTSIN,
  INTERMIDATE_POSITIONEMBEDQ,
  INTERMIDATE_POSITIONEMBEDK,
  INTERMIDATE_SELFOUT,
  INTERMIDATE_SELFLINEAROUT,
  INTERMIDATE_SELFRESIDUALADDOUT,
  INTERMIDATE_SELFNORMOUT,
  INTERMIDATE_MLPOUT,
  INTERMIDATE_MLPLINEARPARALLELOUT,
};

struct LlamaLayerFusionParallelParam
{
  float rmsNormEps = 0;
  int headNum = 0;
  int dk = 0;
  int rank = 0;
  int rankSize = 1;
  std::string model = "llama13b";
  int layerId = 0;
  int rotaryCoeff = 2;
  atb::SVector<int32_t> tokenOffset;
  atb::SVector<int32_t> seqLen;
  bool useCommExt = false;
  void *commExt = nullptr; // only effect when useCommExt is true
};

atb::Status LlamaLayerFusionParallelOperation(const LlamaLayerFusionParallelParam &param,
                                              atb::Operation **operation);

